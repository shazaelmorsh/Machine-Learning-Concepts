{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "\n",
    "buff = [0] * 100 \n",
    "\n",
    "buff_sample = random.sample(buff,10)\n",
    "\n",
    "print(buff_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.], shape=(100,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np_arr = np.zeros(100)\n",
    "print(np_arr)\n",
    "s = tf.convert_to_tensor(np_arr,tf.float32)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1000, score: 16, e: 1.0\n",
      "episode: 1/1000, score: 39, e: 1.0\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.09397387 -0.76313007  0.07319549  1.1415876 ]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.10923646 -0.5690371   0.09602724  0.8727279 ]]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[0.11716747 0.15467612 0.11186315 0.9555584 ]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[0.120261   0.3481302  0.13097432 0.7000102 ]]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[ 0.05523582  0.94115067  0.07186548 -0.3533125 ]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[ 0.07405884  0.7450843   0.06479923 -0.03886184]]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.14290428 -0.18977392  0.15924773  0.5325201 ]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.14669976  0.00279216  0.16989812  0.29394704]]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.04823403 -0.5670598   0.00587451  0.8257928 ]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.05957523 -0.7622616   0.02239036  1.1203176 ]]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.06424566 -0.43374512  0.03122534  0.60671425]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.07292057 -0.62928945  0.04335963  0.9090665 ]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.02478978  1.5302238   0.13172872 -1.3262066 ]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[ 0.0058147   1.3337079   0.1052046  -0.99537015]]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.13933633 -0.18558675  0.14657947  0.43826804]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.14304806  0.00718948  0.15534483  0.19514477]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[ 0.12934753 -0.24202535  0.1928992   1.7049158 ]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[ 0.12450702 -0.43877292  0.22699751  2.0509245 ]]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.12763129  0.37704772  0.19293846  0.05811698]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.12009033  0.5689552   0.1941008  -0.16803032]]\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[ 0.0058147   1.3337079   0.1052046  -0.99537015]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[ 0.03248886  1.1373483   0.0852972  -0.671586  ]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.17142902 -0.8373839   0.203182    1.5143048 ]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.1881767 -1.0343021  0.2334681  1.8629314]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.05153822  1.3374219   0.15343542 -1.0853344 ]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.02478978  1.5302238   0.13172872 -1.3262066 ]]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.14669976  0.00279216  0.16989812  0.29394704]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.14664392  0.19513604  0.17577706  0.05929535]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.14975008 -0.4435086   0.16312312  0.8318622 ]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.15862025 -0.64043903  0.17976035  1.1710823 ]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.11461965 -0.4368273   0.10468758  0.67775184]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.12335619 -0.24330357  0.11824261  0.41977742]]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.13897716  0.38227364  0.18839632 -0.05929975]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.13133168  0.18502018  0.18721032  0.28640655]]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.09349431  0.9528219   0.18286513 -0.6207566 ]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.07443787  1.1449828   0.17044999 -0.85072875]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.0430011  -0.43354407  0.00127986  0.60209423]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.05167199 -0.6286839   0.01332174  0.89518005]]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.14664392  0.19513604  0.17577706  0.05929535]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.14274119 -0.00201346  0.17696297  0.40187553]]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.07443787  1.1449828   0.17044999 -0.85072875]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.05153822  1.3374219   0.15343542 -1.0853344 ]]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.03258761 -0.23873402 -0.01166557  0.31631854]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.03736229 -0.04344787 -0.0053392   0.01997964]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.13133168  0.18502018  0.18721032  0.28640655]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.12763129  0.37704772  0.19293846  0.05811698]]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.14274119 -0.00201346  0.17696297  0.40187553]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.14278147  0.19021508  0.18500048  0.16979179]]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.12812407 -0.1819746   0.12571608  0.3568191 ]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.13176356 -0.37863877  0.13285245  0.6863505 ]]\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.13701996 -0.63650554  0.14158362  1.0769743 ]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.14975008 -0.4435086   0.16312312  0.8318622 ]]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[0.1272236  0.15145917 0.14497453 1.0308865 ]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[ 0.13025278 -0.04526271  0.16559225  1.3653471 ]]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.07482047 -0.95767     0.04479671  1.4199388 ]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.09397387 -0.76313007  0.07319549  1.1415876 ]]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.05957523 -0.7622616   0.02239036  1.1203176 ]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.07482047 -0.95767     0.04479671  1.4199388 ]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.12822227 -0.43988508  0.12663816  0.74727315]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.13701996 -0.63650554  0.14158362  1.0769743 ]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[-0.08550635 -0.8249706   0.06154096  1.2150561 ]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.10200576 -0.63069415  0.08584208  0.9422748 ]]\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "State type: <class 'numpy.ndarray'>\n",
      "State: [[array([-0.03171182, -0.04378937, -0.01221587,  0.02751475], dtype=float32)\n",
      "  {}]]\n",
      "Next state type: <class 'numpy.ndarray'>\n",
      "Next state: [[-0.03258761 -0.23873402 -0.01166557  0.31631854]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m     agent\u001b[38;5;241m.\u001b[39mtrain(env, episodes)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 103\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[70], line 100\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[1;32m     99\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# Number of episodes for training\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain(env, episodes)\n",
      "Cell \u001b[0;32mIn[70], line 87\u001b[0m, in \u001b[0;36mDQN.train\u001b[0;34m(self, env, episodes)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, score: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, e: \u001b[39m\u001b[38;5;132;01m{:.2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e, episodes, time, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon))\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay()\n",
      "Cell \u001b[0;32mIn[70], line 59\u001b[0m, in \u001b[0;36mDQN.replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNext state type:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(next_state))\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNext state:\u001b[39m\u001b[38;5;124m\"\u001b[39m, next_state)\n\u001b[0;32m---> 59\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(state, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     60\u001b[0m target_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(state_tensor)\n\u001b[1;32m     61\u001b[0m target_f[\u001b[38;5;241m0\u001b[39m][action] \u001b[38;5;241m=\u001b[39m target\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mEagerTensor(value, ctx\u001b[38;5;241m.\u001b[39mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import gym\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, batch_size=32):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = []\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Simple Q-network with fully connected layers\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # Strip any additional metadata or nested structures from next_state\n",
    "        next_state = np.array(next_state)  # Ensure next_state is a NumPy array\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        print(\"State shape:\", state.shape)\n",
    "        print(\"State:\", state)\n",
    "        state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        act_values = self.model.predict(state_tensor)\n",
    "        # act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states, targets = [], []\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state_tensor = tf.convert_to_tensor(next_state, dtype=tf.float32)\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state_tensor)[0]))\n",
    "            print(\"State type:\", type(state))\n",
    "            print(\"State:\", state)\n",
    "            print(\"Next state type:\", type(next_state))\n",
    "            print(\"Next state:\", next_state)\n",
    "            state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "            target_f = self.model.predict(state_tensor)\n",
    "            target_f[0][action] = target\n",
    "            states.append(state[0])\n",
    "            targets.append(target_f[0])\n",
    "        states_tensor = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        targets_tensor = tf.convert_to_tensor(targets, dtype=tf.float32)\n",
    "        self.model.fit(states_tensor, targets_tensor, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, env, episodes):\n",
    "        for e in range(episodes):\n",
    "            state = env.reset()\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "            for time in range(500):  # Limiting the number of steps per episode to 500\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _ = env.step(action)[:4]\n",
    "                reward = reward if not done else -10  # Penalty for termination\n",
    "                next_state = np.expand_dims(next_state, axis=0)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, episodes, time, self.epsilon))\n",
    "                    break\n",
    "            self.replay()\n",
    "\n",
    "def main():\n",
    "    # Initialize environment\n",
    "    env = gym.make('CartPole-v1')  # Example environment (CartPole)\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    \n",
    "    # Initialize DQN agent\n",
    "    agent = DQN(state_size, action_size)\n",
    "    \n",
    "    # Train the agent\n",
    "    episodes = 1000  # Number of episodes for training\n",
    "    agent.train(env, episodes)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
